# lct-2025-task-6-intelligent-digital-data-engineer

–°—Å—ã–ª–∫–∞ –Ω–∞ –¥–∞–Ω–Ω—ã–µ: https://huggingface.co/datasets/ArtiomNosov/lct-2025-task-6-intelligent-digital-data-engineer-dataset

# Cursor rules

- [add_commit](https://github.com/ArtiomNosov/hackathon-starter-pack/blob/main/.cursor/rules/commit.mdc)
- [init_task](https://github.com/ArtiomNosov/hackathon-starter-pack/blob/main/.cursor/rules/task.mdc)

# üöÄ Intelligent Digital Data Engineer ‚Äì ETL Platform

## üìñ –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ—Ç–æ—Ç–∏–ø ETL-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –Ω–∞ –±–∞–∑–µ **Big Data** –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.  
–†–µ—à–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –≤ —Ä–∞–º–∫–∞—Ö —Ö–∞–∫–∞—Ç–æ–Ω–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Ç–æ–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, –∏—Ö –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è.  

–û—Å–Ω–æ–≤–Ω—ã–µ —Ü–µ–ª–∏:
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–∞ Docker.
- –ü–æ–¥–Ω—è—Ç—å **Airflow** –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ ETL-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å **PostgreSQL** –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
- –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å **Apache Kafka** –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å **Apache Spark** –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
- –°–æ–∑–¥–∞—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å DAG‚Äô–∏ (—Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã).

---

## üõ† –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
- üêò **PostgreSQL** ‚Äî —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö  
- üå¨ **Apache Airflow** ‚Äî –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è ETL –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
- ‚ö° **Apache Kafka** ‚Äî –ø–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- üóÇÔ∏è **Apache Zookeeper** ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è Kafka –∫–ª–∞—Å—Ç–µ—Ä–∞
- üî• **Apache Spark** ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
- üê≥ **Docker / Rancher Desktop** ‚Äî –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è  
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) üìä **Grafana + Prometheus** ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥  

---

## ‚ö° –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Docker (Rancher Desktop)  
[–°–∫–∞—á–∞—Ç—å Rancher Desktop](https://rancherdesktop.io/)

---

### 2. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
```bash
git clone https://github.com/<your_team_repo>.git
cd lct-2025-task-6-intelligent-digital-data-engineer/docker
```

---

### 3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å—ã
```bash
docker compose up -d
```

---

### 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Airflow (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫)
```bash
docker compose run --rm airflow-webserver bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
```

–ó–∞—Ç–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å—ã:
```bash
docker compose up -d
```

---

### 5. –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–∏—Å–∞–º

- üåê **Airflow UI**: http://localhost:8081
  - –õ–æ–≥–∏–Ω: admin
  - –ü–∞—Ä–æ–ª—å: admin

- üêò **PostgreSQL**:
  - –•–æ—Å—Ç: localhost
  - –ü–æ—Ä—Ç: 5432
  - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: admin
  - –ü–∞—Ä–æ–ª—å: admin
  - –ë–∞–∑–∞: etl_db 

- ‚ö° **Apache Kafka**:
  - –•–æ—Å—Ç: localhost
  - –ü–æ—Ä—Ç: 9092
  - –ë—Ä–æ–∫–µ—Ä: PLAINTEXT://localhost:9092

- üóÇÔ∏è **Apache Zookeeper**:
  - –•–æ—Å—Ç: localhost
  - –ü–æ—Ä—Ç: 2181

- üî• **Apache Spark**:
  - **Spark Master UI**: http://localhost:8082
  - **Spark Worker UI**: http://localhost:8083
  - **Master URL**: spark://localhost:7077
---
## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### Apache Spark
```bash
# –ó–∞–π—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä Spark Master
docker exec -it spark-master bash

# –ó–∞–ø—É—Å—Ç–∏—Ç—å PySpark
/opt/spark/bin/pyspark --master spark://spark-master:7077

# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –≤ PySpark
sc.parallelize([1, 2, 3, 4, 5]).collect()
df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
df.show()
```

## üß™ Apache Kafka
```bash
# –ó–∞–π—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä Kafka
docker exec -it kafka bash

# –°–æ–∑–¥–∞—Ç—å —Ç–æ–ø–∏–∫
kafka-topics --create --topic test-topic --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Å—é–º–µ—Ä–∞ (—Ç–µ—Ä–º–∏–Ω–∞–ª 1)
kafka-console-consumer --topic test-topic --bootstrap-server kafka:9092 --from-beginning

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–¥—é—Å–µ—Ä–∞ (—Ç–µ—Ä–º–∏–Ω–∞–ª 2)
kafka-console-producer --topic test-topic --bootstrap-server kafka:9092
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
```bash
# –°–ø–∏—Å–æ–∫ —Ç–æ–ø–∏–∫–æ–≤
kafka-topics --list --bootstrap-server kafka:9092

# –û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–∞
kafka-topics --describe --topic test-topic --bootstrap-server kafka:9092

# Spark –∑–∞–¥–∞—á–∏
/opt/spark/bin/spark-submit --master spark://spark-master:7077 --class org.apache.spark.examples.SparkPi /opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar 10
```

---

## ‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. DAG "Hello World"

–í –ø–∞–ø–∫–µ dags/ –µ—Å—Ç—å –ø—Ä–∏–º–µ—Ä DAG:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id="hello_world",
    start_date=datetime(2025, 10, 1),
    schedule="@once",
    catchup=False,
) as dag:
    task = BashOperator(
        task_id="print_hello",
        bash_command="echo 'Hello from Airflow!'"
    )
```

### 2. DAG —Å Spark –∑–∞–¥–∞—á–∞–º–∏
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    'simple_spark_test',
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=['spark', 'test'],
) as dag:

    spark_hello_world = BashOperator(
        task_id='spark_hello_world',
        bash_command="""
        docker exec spark-master /opt/spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            --class org.apache.spark.examples.SparkPi \
            /opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar 10
        """,
    )
```

---

## üîß –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–∞–º–∏

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
```bash
docker compose ps
```

### –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
```bash
# Airflow webserver
docker logs airflow-webserver --tail 100

# Airflow scheduler  
docker logs airflow-scheduler --tail 100

# Kafka
docker logs kafka --tail 100

# Zookeeper
docker logs zookeeper --tail 100

# Spark Master
docker logs spark-master --tail 100

# Spark Worker
docker logs spark-worker --tail 100
```

### –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤
```bash
docker compose down
```

### –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (–≤–∫–ª—é—á–∞—è –¥–∞–Ω–Ω—ã–µ)
```bash
docker compose down -v
```

---
## üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ETL –ø–∞–π–ø–ª–∞–π–Ω–∞

**–ü–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö:**
  - Data Sources ‚Üí Apache Kafka ‚Üí Apache Spark ‚Üí PostgreSQL Database
  - ‚Üë ‚Üë
  - Apache Zookeeper Spark Master
  - ‚Üì
  - Spark Worker

–í—Å—ë —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ Apache Airflow

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**
- **Data Sources** ‚Üí **Kafka** ‚Üí **Spark** ‚Üí **PostgreSQL**
- **Airflow** —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º
- **Zookeeper** –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç Kafka
- **Spark Master** —É–ø—Ä–∞–≤–ª—è–µ—Ç Worker'–∞–º–∏

---

### üë• –ö–æ–º–∞–Ω–¥–∞
- –§–ò–û / Telegram / GitHub —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∫–æ–º–∞–Ω–¥—ã

---
## üìå –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

- DAG‚Äô–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ –ø–∞–ø–∫—É dags/ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞—é—Ç—Å—è Airflow).
- Kafka —Ç–æ–ø–∏–∫–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏.
- Spark –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –∫–∞–∫ —á–µ—Ä–µ–∑ Airflow DAG'–∏, —Ç–∞–∫ –∏ –Ω–∞–ø—Ä—è–º—É—é.
- –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ Docker volumes –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏.

### –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –≤—Ä—É—á–Ω—É—é:
```bash
docker compose run --rm airflow-webserver 
    airflow users create 
    --username admin 
    --password admin 
    --firstname Admin 
    --lastname User 
    --role Admin 
    --email admin@example.com
```

### –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:
```bash
# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
docker compose restart

# –ü—Ä–æ—Å–º–æ—Ç—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
docker stats

# –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –æ–±—Ä–∞–∑–æ–≤
dock
```

# üìä –ü—Ä–∏–º–µ—Ä: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö –≤ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ

## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞
–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ç—Ä–µ—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö:
- **CSV**: –ú—É–∑–µ–π–Ω—ã–µ –±–∏–ª–µ—Ç—ã (~4GB, 12 —Ñ–∞–π–ª–æ–≤)
- **JSON**: –†–µ–µ—Å—Ç—Ä –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π (~8.3GB, 31 —Ñ–∞–π–ª)  
- **XML**: –ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (~11.8GB, 15 —Ñ–∞–π–ª–æ–≤)

## –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```bash
# –ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–∏ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–ø–∫—É data analysis/
mkdir -p "data analysis"
# –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç—É–¥–∞ –≤–∞—à–∏ CSV, JSON, XML —Ñ–∞–π–ª—ã
```

## –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ DAG –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `dags/multi_format_analysis.py`:

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta
import json

default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def create_analysis_script():
    """–°–æ–∑–¥–∞–Ω–∏–µ Spark —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    spark_code = '''
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

spark = SparkSession.builder \\
    .appName("DataAnalysis") \\
    .master("spark://spark-master:7077") \\
    .getOrCreate()

print("üî• –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö...")

# 1. –ê–Ω–∞–ª–∏–∑ CSV –¥–∞–Ω–Ω—ã—Ö (–ú—É–∑–µ–π–Ω—ã–µ –±–∏–ª–µ—Ç—ã)
csv_df = spark.read \\
    .option("header", "true") \\
    .option("inferSchema", "true") \\
    .csv("/opt/airflow/dags/data_analysis/*.csv")

# –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞
csv_cleaned = csv_df \\
    .withColumn("price_valid", when(col("ticket_price") > 0, col("ticket_price")).otherwise(0)) \\
    .withColumn("phone_valid", when(col("client_phone").rlike("^[78]\\d{10}$"), "–í–ï–†–ù–´–ô").otherwise("–ù–ï–í–ï–†–ù–´–ô")) \\
    .withColumn("data_source", lit("museum_csv"))

# –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –º—É–∑–µ—è–º
museum_stats = csv_cleaned.groupBy("museum_name") \\
    .agg(
        count("*").alias("total_tickets"),
        sum("price_valid").alias("total_revenue"),
        avg("price_valid").alias("avg_price")
    )

print(f"‚úÖ CSV: {csv_cleaned.count()} –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")

# 2. –ê–Ω–∞–ª–∏–∑ JSON –¥–∞–Ω–Ω—ã—Ö (–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏)
json_files = spark.sparkContext.wholeTextFiles("/opt/airflow/dags/data_analysis/*.json")
json_rdd = json_files.flatMap(lambda x: json.loads(x[1]) if isinstance(json.loads(x[1]), list) else [json.loads(x[1])])
json_df = spark.createDataFrame(json_rdd)

# –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—è–º
entrepreneur_stats = json_df.groupBy("inf_authority_reg_ind_entrep_name") \\
    .agg(count("*").alias("total_registrations"))

print(f"‚úÖ JSON: {json_df.count()} –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")

# 3. –ê–Ω–∞–ª–∏–∑ XML –¥–∞–Ω–Ω—ã—Ö (–ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ)
xml_df = spark.read \\
    .format("xml") \\
    .option("rowTag", "item") \\
    .load("/opt/airflow/dags/data_analysis/*.xml")

# –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –∫–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
cadastral_stats = xml_df.groupBy("type") \\
    .agg(count("*").alias("total_objects"))

print(f"‚úÖ XML: {xml_df.count()} –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ PostgreSQL
museum_stats.write \\
    .format("jdbc") \\
    .option("url", "jdbc:postgresql://postgres:5432/etl_db") \\
    .option("dbtable", "museum_analytics") \\
    .option("user", "admin") \\
    .option("password", "admin") \\
    .option("driver", "org.postgresql.Driver") \\
    .mode("overwrite") \\
    .save()

print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ PostgreSQL!")
spark.stop()
'''
    
    with open('/tmp/data_analysis.py', 'w') as f:
        f.write(spark_code)

with DAG(
    'multi_format_analysis',
    default_args=default_args,
    description='–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö CSV, JSON, XML',
    schedule_interval=None,
    catchup=False,
    tags=['analysis', 'csv', 'json', 'xml'],
) as dag:

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
    create_tables = PostgresOperator(
        task_id='create_tables',
        postgres_conn_id='postgres_default',
        sql="""
        CREATE TABLE IF NOT EXISTS museum_analytics (
            id SERIAL PRIMARY KEY,
            museum_name VARCHAR(500),
            total_tickets INTEGER,
            total_revenue DECIMAL(15,2),
            avg_price DECIMAL(10,2),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """,
    )

    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    copy_data = BashOperator(
        task_id='copy_data',
        bash_command="""
        docker exec spark-master mkdir -p /opt/airflow/dags/data_analysis
        docker cp "data analysis/" spark-master:/opt/airflow/dags/
        echo "‚úÖ –î–∞–Ω–Ω—ã–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã"
        """,
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞
    create_script = PythonOperator(
        task_id='create_script',
        python_callable=create_analysis_script,
    )

    # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
    run_analysis = BashOperator(
        task_id='run_analysis',
        bash_command="""
        docker exec spark-master /opt/spark/bin/spark-submit \\
            --master spark://spark-master:7077 \\
            --packages org.postgresql:postgresql:42.7.0,com.databricks:spark-xml_2.12:0.15.0 \\
            --executor-memory 1g \\
            /tmp/data_analysis.py
        """,
    )

    # –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    view_results = BashOperator(
        task_id='view_results',
        bash_command="""
        echo "üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:"
        docker exec postgres psql -U admin -d etl_db -c "
        SELECT * FROM museum_analytics LIMIT 10;
        "
        """,
    )

    create_tables >> copy_data >> create_script >> run_analysis >> view_results
```

## –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞

1. **–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ñ–∞–π–ª** `multi_format_analysis.py` –≤ –ø–∞–ø–∫—É `dags/`
2. **–ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ** –≤ –ø–∞–ø–∫—É `data analysis/`
3. **–í Airflow UI** (http://localhost:8081):
   - –ù–∞–π–¥–∏—Ç–µ DAG `multi_format_analysis`
   - –í–∫–ª—é—á–∏—Ç–µ –µ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª–µ–º
   - –ù–∞–∂–º–∏—Ç–µ **"Trigger DAG"** ‚ñ∂Ô∏è

## –®–∞–≥ 4: –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –í Airflow UI:
- –°–ª–µ–¥–∏—Ç–µ –∑–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∑–∞–¥–∞—á –≤ Graph View
- –ö–ª–∏–∫–Ω–∏—Ç–µ –Ω–∞ –∑–∞–¥–∞—á—É `view_results` –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –í PostgreSQL:
```bash
# –ü–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
docker exec -it postgres psql -U admin -d etl_db

# –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
SELECT * FROM museum_analytics ORDER BY total_revenue DESC LIMIT 10;
```

## –ß—Ç–æ –ø–æ–ª—É—á–∏—Ç–µ:

‚úÖ **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º—É–∑–µ—è–º** (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–ª–µ—Ç–æ–≤, –≤—ã—Ä—É—á–∫–∞, —Å—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞)  
‚úÖ **–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π** (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–π –ø–æ –æ—Ä–≥–∞–Ω–∞–º)  
‚úÖ **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–¥–∞—Å—Ç—Ä–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤** (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ —Ç–∏–ø–∞–º)  
‚úÖ **–í–∞–ª–∏–¥–∞—Ü–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö** (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏)  
‚úÖ **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ PostgreSQL** –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞  

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ:

1. **–ó–∞–º–µ–Ω–∏—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º** –≤ –∫–æ–¥–µ –Ω–∞ –≤–∞—à–∏
2. **–ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–ª—è** –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –≤–∞–ª–∏–¥–∞—Ü–∏—é** –ø–æ–¥ –≤–∞—à–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
4. **–î–æ–±–∞–≤—å—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏** –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:

- [CSV Files] ‚îÄ‚îÄ‚îê
- [JSON Files] ‚îÄ‚îº‚îÄ‚ñ∫ [Spark Ingestion] ‚îÄ‚îÄ‚ñ∫ [Data Validator] ‚îÄ‚îÄ‚ñ∫ [Analytics]
- [XML Files] ‚îÄ‚îÄ‚îò ‚îÇ
‚ñº
- [PostgreSQL] ‚óÑ‚îÄ‚îÄ [Results Aggregator] ‚óÑ‚îÄ‚îÄ [Quality Checker]

**–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏!** üöÄ
